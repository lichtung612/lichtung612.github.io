<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>扩散模型（九）| Diffusion for Point Cloud | 披星戴月思想家</title><meta name="author" content="梦游娃娃"><meta name="copyright" content="梦游娃娃"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="PointE PointE: A system for Generating 3D Point Clouds from Complex Prompts https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2212.08751.pdf https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;point-e  Abstract最近的text-conditional 3D生成模型经常需要多个GPU-hours来生成">
<meta property="og:type" content="article">
<meta property="og:title" content="扩散模型（九）| Diffusion for Point Cloud">
<meta property="og:url" content="http://example.com/posts/9-diffusion-models/index.html">
<meta property="og:site_name" content="披星戴月思想家">
<meta property="og:description" content="PointE PointE: A system for Generating 3D Point Clouds from Complex Prompts https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2212.08751.pdf https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;point-e  Abstract最近的text-conditional 3D生成模型经常需要多个GPU-hours来生成">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/image/avatar.jpg">
<meta property="article:published_time" content="2024-01-23T16:00:00.000Z">
<meta property="article:modified_time" content="2024-05-22T11:41:33.155Z">
<meta property="article:author" content="梦游娃娃">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/image/avatar.jpg"><link rel="shortcut icon" href="/image/favicon.ico"><link rel="canonical" href="http://example.com/posts/9-diffusion-models/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '扩散模型（九）| Diffusion for Point Cloud',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-05-22 19:41:33'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/image/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">16</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/image/cover.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="披星戴月思想家"><span class="site-name">披星戴月思想家</span></a></span><div id="menus"><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">扩散模型（九）| Diffusion for Point Cloud</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-01-23T16:00:00.000Z" title="发表于 2024-01-24 00:00:00">2024-01-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-05-22T11:41:33.155Z" title="更新于 2024-05-22 19:41:33">2024-05-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Diffusion-model/">Diffusion model</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="扩散模型（九）| Diffusion for Point Cloud"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="PointE"><a href="#PointE" class="headerlink" title="PointE"></a>PointE</h2><blockquote>
<p>PointE: A system for Generating 3D Point Clouds from Complex Prompts</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2212.08751.pdf">https://arxiv.org/pdf/2212.08751.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/openai/point-e">https://github.com/openai/point-e</a></p>
</blockquote>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>最近的text-conditional 3D生成模型经常需要多个GPU-hours来生成一个单一样本，这和图像生成模型（几秒或几分钟就可以生成样本）相比有巨大的差距。本文提出一个3D生成模型PointE（generate Point clouds Efficiently)，可以用单个GPU在1-2分钟内生成3D模型。</p>
<p>该方法首先用一个text-to-image diffusion模型生成一个单一的合成视角的图像，之后用另一个输入条件为图像的diffusion模型生成3D点云。</p>
<p>该方法虽然质量上和sota模型有差距，但是速度比其他的快1-2个数量级。</p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>最近的text-to-3D方法大致分成2类：</p>
<ul>
<li>使用成对的数据（text，3D）或者没有标签的3D数据训练生成模型。这些方法利用现有的生成模型方法来高效生成样本，但是它们面对生成多样性和复杂的文本Prompt是困难的，因为目前缺乏大规模3D数据集。</li>
<li>利用预训练的text-image模型来优化3D表示。这些方法可以处理复杂多样的text prompts，但是需要昂贵的优化过程来生成样本。此外，由于缺乏3D先验知识，这些方法容易陷入局部最优，不能生成有意义或连贯的三维对象。</li>
</ul>
<p>本文试图结合两种方法的优势，通过将text-to-image模型和image-to-3D模型结合起来。其中，text-to-image模型利用大量的(text,image)语料对，允许模型根据多样复杂的prompt进行生成；image-to-3D模型在一个更小的(image,3D)数据集上训练。为了根据text prompt生成一个3D物体，首先使用text-to-image模型采样一个图像，之后根据这个采样的图像使用image-to-3D模型采样一个3D object。</p>
<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p><img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/7-diffusion-models/0.jpg" alt="img"></p>
<ol>
<li>通过一个text caption生成一个合成视角的图像 &lt;- 3-billion参数的在渲染的3D模型数据集上微调的GLIDE模型</li>
<li>通过合成视角的图像生成一个粗粒度的点云（1024points) &lt;-具有置换不变性的diffusion model</li>
<li>在低分辨率点云和合成视角图像条件下进行进一步生成，生成一个细粒度的点云（4096points）&lt;-和第二个diffusion模型相似但是更小的扩散模型</li>
</ol>
<h4 id="Synthesis-GLIDE-Model"><a href="#Synthesis-GLIDE-Model" class="headerlink" title="Synthesis GLIDE Model"></a>Synthesis GLIDE Model</h4><p>为了确保text-to-image模型可以生成正确的合成视角，训练模型让模型可以生成满足点云训练数据集分布的3D渲染。</p>
<p>为此，使用GLIDE初始训练数据集和3D渲染数据集微调GLIDE模型。因为3D渲染数据集相比于GLIDE训练数据集小很多，仅仅只在5%的时间内对3D数据集的图像进行采样，其余95%使用原始数据集。微调100K个迭代，意味着模型已经在整个3D数据集上训练了几个epochs。</p>
<p>在测试时，为了确保我们总是在3D分布渲染中采样，我们在每个3D渲染的文本提示中添加一个特殊的标记，指示它是3D渲染；然后在测试时使用此token进行采样。</p>
<h4 id="Point-Cloud-Diffusion"><a href="#Point-Cloud-Diffusion" class="headerlink" title="Point Cloud Diffusion"></a>Point Cloud Diffusion</h4><img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/7-diffusion-models/1.jpg" alt="img" style="zoom:50%;" />

<p>将点云表示成一个tensor，shape为K x 6，K是点云中点的数量，特征维度为(x,y,z,R,G,B)。全部坐标和颜色被标准化为[-1,1]。输入随机噪声K x 6，通过diffusion模型逐步denoising，生成点云。</p>
<p>利用简单的Transformer-based模型基于输入图像、时间步t、噪声点云 $x_t$来预测噪声 $\epsilon$和方差 Σ。 </p>
<ul>
<li>条件编码<ul>
<li>t：通过一个小的MLP，获得一个D维向量</li>
<li>噪声点云：每个点通过一个linear层，得到K x D</li>
<li>图像：使用一个预训练的ViT-L/14 CLIP模型提取特征，选择最后一层的特征嵌入（256 x $D’$），线性投影成256 x D</li>
</ul>
</li>
</ul>
<p>所有条件编码在batch维度concat，构成transformer的输入：(K+257)xD。为了获得长度为K的最终输出序列，我们取输出的最后K个标记并将其投影，以获得K个输入点的ε和∑预测。</p>
<p>注意，因为在整个过程中都没有使用位置编码，所以模型满足置换不变性。</p>
<h4 id="Point-Cloud-Upsampler"><a href="#Point-Cloud-Upsampler" class="headerlink" title="Point Cloud Upsampler"></a>Point Cloud Upsampler</h4><p>upsampler模型采用和base模型相同的架构。对于低分辨率的point cloud采用另外的condition tokens，将条件点云通过一个分离的线性嵌入层，使模型可以区分高分辨率点云和低分辨率点云的信息。</p>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><h4 id="Qualitative-Results"><a href="#Qualitative-Results" class="headerlink" title="Qualitative Results"></a>Qualitative Results</h4><p>PointE可以基于复杂的Prompts生成高质量的3D shapes。</p>
<img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/7-diffusion-models/2.jpg" alt="img" style="zoom:50%;" />

<p>失败的例子：</p>
<ol>
<li>错误地理解不同部分的相对比例，导致生成了一个高的狗而非短的长的狗。</li>
<li>不能推理出被遮挡的部分</li>
</ol>
<img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/7-diffusion-models/3.jpg" alt="img" style="zoom:50%;" />

<h4 id="Model-Scaling-and-Ablations"><a href="#Model-Scaling-and-Ablations" class="headerlink" title="Model Scaling and Ablations"></a>Model Scaling and Ablations</h4><ol>
<li>仅仅使用text conditioning，而没有text-to-image步骤导致模型产生更差的结果</li>
<li>使用一个单一的token编码图像CLIP embedding比使用多个token编码CLIP embedding产生更差的结果</li>
<li>扩大模型可以加速收敛，增大CLIP R-Presicion结果。</li>
</ol>
<img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/7-diffusion-models/4.jpg" alt="img" style="zoom:67%;" />

<h2 id="PC2"><a href="#PC2" class="headerlink" title="PC2"></a>PC2</h2><blockquote>
<p>$PC^2$：Projection-Conditioned Point Cloud Diffusion for Single-Image 3D Reconstruction（牛津-CVPR23）</p>
<p>项目主页：<a target="_blank" rel="noopener" href="https://lukemelas.github.io/projection-conditioned-point-cloud-diffusion/">https://lukemelas.github.io/projection-conditioned-point-cloud-diffusion/</a></p>
<p>补充材料：<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Melas-Kyriazi_PC2_Projection-Conditioned_Point_CVPR_2023_supplemental.pdf">https://openaccess.thecvf.com/content/CVPR2023/supplemental/Melas-Kyriazi_PC2_Projection-Conditioned_Point_CVPR_2023_supplemental.pdf</a></p>
<p>代码：<a target="_blank" rel="noopener" href="https://github.com/lukemelas/projection-conditioned-point-cloud-diffusion">https://github.com/lukemelas/projection-conditioned-point-cloud-diffusion</a></p>
</blockquote>
<p><img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/7-diffusion-models/5.jpg" alt="img"></p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><em><strong>任务：</strong></em>Single-View 3D reconstruction，给定一张RGB图片和对应视角，输出该图片中物体的3D点云。</p>
<p><em><strong>创新：</strong></em>（1）以往的single-view 3D reconstruction生成的可能是体素或者NeRF表示，本文是第一个生成点云形式表示的工作。（2）首次将扩散模型应用到该任务中（3）条件注入方式采用投影注入(projection-conditioned)，在预测点的位置信息之后，再次使用投影条件注入方式来预测点的颜色（4）引入过滤步骤，利用diffusion生成过程的概率性，解决单视角3D重构问题中的不适应性(ill-posed nature）</p>
<p><em><strong>实验：</strong></em>（1）对比之前的工作仅仅在合成数据集setting上进行实验，该方法不仅在合成数据集ShapeNet上达到SOTA；在复杂的真实世界数据集Co3D上也进行实验，性能达到SOTA。（2）可视化结果来看，该方法从任何视角都能产生真实的物体形态，可以生成具有更精细细节级别的形状。（3）消融实验证明了投影注入和过滤步骤的有效性</p>
<h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><h4 id="Single-View-3D-reconstruction"><a href="#Single-View-3D-reconstruction" class="headerlink" title="Single-View 3D reconstruction"></a>Single-View 3D reconstruction</h4><ul>
<li>3D-R2N2：标准2D卷积网络编码图像，3D-LSTM处理图像特征，3D卷积网络解码成一个<strong>体素形式</strong>3D物体。</li>
<li>另一种范式是使用NeRF来学习隐式表示，如Nerf-WCE和PixelNeRF。这两种方法在few-view setting中表现不错，但是在single-view setting中表现欠佳。</li>
</ul>
<p>本论文使用了一个完全不同的方法，利用扩散模型来进行3D重构。由于扩散模型的概率特性（probabilistic nature），使我们捕捉到不可见区域的模糊性，生成高分辨率点云形状。</p>
<h4 id="Diffusion-Models-for-Point-Clouds"><a href="#Diffusion-Models-for-Point-Clouds" class="headerlink" title="Diffusion Models for Point Clouds"></a>Diffusion Models for Point Clouds</h4><p>在过去一年中，扩散模型应用到<strong>无条件<strong><strong>点云</strong></strong>生成任务</strong>上的方法被提出。</p>
<p>diffusion-point-cloud和 Point-Voxel Diffusion (PVD)提出了相似的生成范式，不同的是它们的backbone一个是PointNet，一个是Point-Voxel-CNN。Point Diffusion-Refinement (PDR) 使用扩散模型来解决点云补全任务。</p>
<p>然而，这三个工作仅仅解决无条件的形状生成问题或者补全问题，没有解决从2D图片中进行3D重构的问题。此外，它们仅仅在合成数据集上训练，本文还展示了在真实世界数据上的结果。</p>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>如下图所示，输入一张RGB图片和它的camera pose，从一个三维高斯分布随机采样一组点，这组点根据RGB图片和camera pose条件在diffusion过程中逐渐降噪，最终得到图片中物体的3D点云。具体来说，在扩散过程的每一步，图片特征和camera pose被投影到点云中，使生成的点云能够很好地和输入图像对齐，具备几何一致性。随后，使用相同的投影方法预测点的颜色。</p>
<p>因为diffusion的概率特性，可以针对一张图片生成多个可信的点云，引入一个过滤步骤来帮助解决单视角3D重构中的不适应性。具体来说，根据给定的图像生成多个点云，通过它们和输入mask的匹配程度来过滤掉某些点云，使生成样本的质量更高。</p>
<p><img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/7-diffusion-models/6.jpg" alt="img"></p>
<h4 id="Point-Cloud-Diffusion-Models"><a href="#Point-Cloud-Diffusion-Models" class="headerlink" title="Point Cloud Diffusion Models"></a>Point Cloud Diffusion Models</h4><p>把具有N个点的3D点云视为一个3N维度的物体，扩散模型 $s_\theta:R^{3N}\rightarrow R^{3N}$，这个网络将一组从一个高斯分布中采样的点逐渐降噪成一个可识别的物体。在每一步，预测当前点的位置和它的ground truth位置的offset，迭代这个过程直到它和目标分布 $q(X_0)$类似。网络 $s_\theta$使用Point-Voxel CNN(PVCNN)。</p>
<p>具体来说，网络被训练来预测噪声 $\epsilon \in R^{3N}$，使用L2 loss来监督：</p>
<p><img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/7-diffusion-models/7.jpg" alt="img"></p>
<p>在推理阶段，从一个三维高斯分布中随机采样点云（<code>x_t = torch.randn(B,N,D）</code>），进行逆向扩散过程生成样本 $X_0$。</p>
<h4 id="条件注入"><a href="#条件注入" class="headerlink" title="条件注入"></a>条件注入</h4><p>目标分布是一个条件分布 $q(X_0|I,V)$,I是输入图像，V是对应的相机视角。</p>
<p>常见的输入图像的条件注入方式是将图像编码成一个global feature，然而这种方法在输入图像和重构shape上呈现弱的几何一致性，实验展示它经常生成可信的shapes，然而这些shapes和输入图像并不匹配。</p>
<p>本文创造PC2（projection-conditional diffusion models）。下图展示global condition和pc2的结果对比，可以看出PC2比global condition在F-scores指标上提高不少。</p>
<p><img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/7-diffusion-models/8.jpg" alt="img"></p>
<p>PC2首先使用一个标准2D图像模型（CNN或者ViT，本文用的是MAE-ViT）提取图像特征，2D图像经过图像模型提取后的特征为 $I \in R^{H\times W \times C}$。</p>
<p>之后，在每一个diffusion步骤之前，将这些特征投影到点云上，投影特征 $X_t^{proj}=P_{V_I}(I,X_t)$，其中 $P_{V_I}$是从相机视角 $V_I$的投影函数，I是输入图像，$X_t$是部分降噪的点云。</p>
<p>经过条件增强的点云为 $X_t^+=[X_t,X_t^{proj}]$，其特征维度为 $R^{(3+C)N}$。点云中每个点得到的投影特征各不相同。</p>
<p>$s_\theta$是预测噪声函数 $\epsilon$: $R^{(3+C)N}\rightarrow R^{3N}$。</p>
<p><strong>投影函数</strong>:</p>
<p>对于投影函数$P_{V_I}(I,X_t)$的设计，一个简单的方式是直接把3D点投影到图片上，拿对应的图片特征。然而，这忽视了点云的立体属性，点云中真面和背面的点得到的特征相同，不符合常识。选择一个<strong>适当考虑自遮挡</strong>的投影函数是较好的。为此作者通过<strong>光栅化</strong>来实现这一过程，由于Pytorch3d中有高度优化的光栅化函数（use the PyTorch3D library for rasterization during the projection conditioning phase），这一过程可以非常高效实现，它只占训练时间和计算量的一小部分。</p>
<h4 id="Coloring-model"><a href="#Coloring-model" class="headerlink" title="Coloring model"></a>Coloring model</h4><p>作者发现可以使用相同的projection-based条件注入方式来重建物体颜色。特别地，本方法学习一个<strong>分离的</strong>coloring模型 $c_\theta:R^{(3+C)N}\rightarrow R^{CN}$，输入经过第一阶段生成的点云，输出每个点的颜色。</p>
<p>PC2的color生成过程利用single-step模型，因为作者发现颜色模型扩散过程中<strong>只需要一步</strong>就可以生成不错的结果，这降低了计算复杂度。</p>
<h4 id="过滤"><a href="#过滤" class="headerlink" title="过滤"></a>过滤</h4><p>由于单视角3D重构问题的ill-posed性质，每张图片有很多可能的ground truth。不同于先前的工作，PC2有概率性的特性，因此可以从一个单一输入图像中采样出多个不同的重构。</p>
<p>过滤过程对某张图片生成多个样本，之后根据某种标准过滤，选择最可信的输出作为最终结果。</p>
<p>提出了2种标准，它们都包含物体剪影（提取3D点云的2D剪影）。一个使用额外的图像监督，一个没有。</p>
<ul>
<li>With mask supervision(PC2-FM)：比较每个点云剪影 $\hat M$和输入图像的主体mask$M$(通过Mask-RCNN提取)。计算两个剪影的交并比，选择具有最高交并比的生成样本。</li>
<li>Without mask supervision(PC2-FA)：根据预测样本之间的相互一致性进行筛选。计算每个预测和其他预测的IoU，选择具有最高平均IoU的预测样本。这种方式可以在不添加任何其他额外监督的情况下选择出较高质量的样本。</li>
</ul>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h4><p>F-score来评估。对于两个点云 $X$和 $\hat X$，和一个固定的阈值距离d，公式为：</p>
<p><img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/7-diffusion-models/9.jpg" alt="img"></p>
<p>其中 $P_d$和 $R_d$表示precision和recall。 $d$是一个固定的阈值距离，本文follow之前的工作，取d=0.01。precision和recall计算公式如下（P(d)：生成点云中的所有点逐个来看，如果真实点云中点和此点的最小距离小于d，将其求和取平均；R(d):真实点云中的所有点逐个来看，如果生成点云中点和此点的最小距离小于d，将其求和取平均）：</p>
<p><img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/7-diffusion-models/10.jpg" alt="img"></p>
<h4 id="Quantitative-Results"><a href="#Quantitative-Results" class="headerlink" title="Quantitative Results"></a>Quantitative Results</h4><p>ShapeNet-R2N2数据集上，没有过滤的情况下，PC2和之前工作差不多性能。通过检查不同类别的性能，我们可以看到PC2在具有更精细的细节相关的对象的类别上表现更好，如“步枪”和“飞机”。在过滤的情况下，PC2-FM达到SOTA结果。</p>
<p><img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/7-diffusion-models/11.jpg" alt="img"></p>
<p><img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/7-diffusion-models/12.jpg" alt="img"></p>
<h4 id="Qualitative-Results-1"><a href="#Qualitative-Results-1" class="headerlink" title="Qualitative Results"></a>Qualitative Results</h4><p>图3和图4展示了PC2在真实世界数据集Co3D上的定性结果。</p>
<p><img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/7-diffusion-models/13.jpg" alt="img"></p>
<p>因为NeRF-WCE的生成是固定的,它很难对高程度不确定的区域进行建模，为远离参考视图的新视图生成模糊的图像。与之对比，PC2从任何视角都能产生真实的物体形态：</p>
<p><img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/7-diffusion-models/14.jpg" alt="img"></p>
<h4 id="Diversity-of-Generations"><a href="#Diversity-of-Generations" class="headerlink" title="Diversity of Generations"></a>Diversity of Generations</h4><p><img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/7-diffusion-models/15.jpg" alt="img"></p>
<h4 id="Filtering-Analysis"><a href="#Filtering-Analysis" class="headerlink" title="Filtering Analysis"></a>Filtering Analysis</h4><p>Oracle根据F-score选择最佳样本，提供了性能上界。PC2-FM相比PC2-FA提升更多。</p>
<p><img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/7-diffusion-models/16.jpg" alt="img"></p>
<p>当过滤不同数量的图片时，性能如下：仅仅使用2张图片进行过滤就可以极大地提升结果，添加额外的图像得到更高的性能提升。增大过滤可选择样本数可以进一步提高性能，但回报递减。</p>
<p><img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/7-diffusion-models/17.jpg" alt="img"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">梦游娃娃</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/posts/9-diffusion-models/">http://example.com/posts/9-diffusion-models/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">披星戴月思想家</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/image/avatar.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/10-diffusion-models/" title="扩散模型（十）| Transformer-based Diffusion：U-ViT"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">扩散模型（十）| Transformer-based Diffusion：U-ViT</div></div></a></div><div class="next-post pull-right"><a href="/posts/8-diffusion-models/" title="扩散模型（八）| ControlNet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">扩散模型（八）| ControlNet</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/image/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">梦游娃娃</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">16</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">24/05/15: Finding an internship is all you need.<br>
<strike>24/02/02: Sleep is all you need.</strike>
</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#PointE"><span class="toc-number">1.</span> <span class="toc-text">PointE</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Abstract"><span class="toc-number">1.1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction"><span class="toc-number">1.2.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Method"><span class="toc-number">1.3.</span> <span class="toc-text">Method</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Synthesis-GLIDE-Model"><span class="toc-number">1.3.1.</span> <span class="toc-text">Synthesis GLIDE Model</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Point-Cloud-Diffusion"><span class="toc-number">1.3.2.</span> <span class="toc-text">Point Cloud Diffusion</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Point-Cloud-Upsampler"><span class="toc-number">1.3.3.</span> <span class="toc-text">Point Cloud Upsampler</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Results"><span class="toc-number">1.4.</span> <span class="toc-text">Results</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Qualitative-Results"><span class="toc-number">1.4.1.</span> <span class="toc-text">Qualitative Results</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Model-Scaling-and-Ablations"><span class="toc-number">1.4.2.</span> <span class="toc-text">Model Scaling and Ablations</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PC2"><span class="toc-number">2.</span> <span class="toc-text">PC2</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E8%A7%88"><span class="toc-number">2.1.</span> <span class="toc-text">概览</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">2.2.</span> <span class="toc-text">相关工作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Single-View-3D-reconstruction"><span class="toc-number">2.2.1.</span> <span class="toc-text">Single-View 3D reconstruction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Diffusion-Models-for-Point-Clouds"><span class="toc-number">2.2.2.</span> <span class="toc-text">Diffusion Models for Point Clouds</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number">2.3.</span> <span class="toc-text">方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Point-Cloud-Diffusion-Models"><span class="toc-number">2.3.1.</span> <span class="toc-text">Point Cloud Diffusion Models</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9D%A1%E4%BB%B6%E6%B3%A8%E5%85%A5"><span class="toc-number">2.3.2.</span> <span class="toc-text">条件注入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Coloring-model"><span class="toc-number">2.3.3.</span> <span class="toc-text">Coloring model</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%87%E6%BB%A4"><span class="toc-number">2.3.4.</span> <span class="toc-text">过滤</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">2.4.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="toc-number">2.4.1.</span> <span class="toc-text">评价指标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Quantitative-Results"><span class="toc-number">2.4.2.</span> <span class="toc-text">Quantitative Results</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Qualitative-Results-1"><span class="toc-number">2.4.3.</span> <span class="toc-text">Qualitative Results</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Diversity-of-Generations"><span class="toc-number">2.4.4.</span> <span class="toc-text">Diversity of Generations</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Filtering-Analysis"><span class="toc-number">2.4.5.</span> <span class="toc-text">Filtering Analysis</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/13-diffusion-models/" title="扩散模型（十三）| DreamBooth">扩散模型（十三）| DreamBooth</a><time datetime="2024-03-03T16:00:00.000Z" title="发表于 2024-03-04 00:00:00">2024-03-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/12-diffusion-models/" title="扩散模型（十二）| LoRA">扩散模型（十二）| LoRA</a><time datetime="2024-02-27T16:00:00.000Z" title="发表于 2024-02-28 00:00:00">2024-02-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/11-diffusion-models/" title="扩散模型（十一）| Transformer-based Diffusion：DiT">扩散模型（十一）| Transformer-based Diffusion：DiT</a><time datetime="2024-02-19T16:00:00.000Z" title="发表于 2024-02-20 00:00:00">2024-02-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/10-diffusion-models/" title="扩散模型（十）| Transformer-based Diffusion：U-ViT">扩散模型（十）| Transformer-based Diffusion：U-ViT</a><time datetime="2024-01-29T16:00:00.000Z" title="发表于 2024-01-30 00:00:00">2024-01-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/9-diffusion-models/" title="扩散模型（九）| Diffusion for Point Cloud">扩散模型（九）| Diffusion for Point Cloud</a><time datetime="2024-01-23T16:00:00.000Z" title="发表于 2024-01-24 00:00:00">2024-01-24</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/image/cover.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By 梦游娃娃</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>