<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>扩散模型（一）| DDPM &amp; DDIM | 披星戴月思想家</title><meta name="author" content="梦游娃娃"><meta name="copyright" content="梦游娃娃"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="学习笔记，主要参考：  https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;666552214 https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v&#x3D;ifCDXFdeaaM(hong-yi Lee Diffusion Model原理剖析） https:&#x2F;&#x2F;www.bilibili.com&#x2F;video&#x2F;BV1Zh411A72y&#x2F;?spm_id_from&#x3D;333.999.0.0&amp;am">
<meta property="og:type" content="article">
<meta property="og:title" content="扩散模型（一）| DDPM &amp; DDIM">
<meta property="og:url" content="http://example.com/posts/1-diffusion-models/index.html">
<meta property="og:site_name" content="披星戴月思想家">
<meta property="og:description" content="学习笔记，主要参考：  https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;666552214 https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v&#x3D;ifCDXFdeaaM(hong-yi Lee Diffusion Model原理剖析） https:&#x2F;&#x2F;www.bilibili.com&#x2F;video&#x2F;BV1Zh411A72y&#x2F;?spm_id_from&#x3D;333.999.0.0&amp;am">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/image/avatar.jpg">
<meta property="article:published_time" content="2024-01-02T16:00:00.000Z">
<meta property="article:modified_time" content="2024-05-07T14:54:59.628Z">
<meta property="article:author" content="梦游娃娃">
<meta property="article:tag" content="DDPM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/image/avatar.jpg"><link rel="shortcut icon" href="/image/favicon.ico"><link rel="canonical" href="http://example.com/posts/1-diffusion-models/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '扩散模型（一）| DDPM & DDIM',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-05-07 22:54:59'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/image/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">14</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/image/cover.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="披星戴月思想家"><span class="site-name">披星戴月思想家</span></a></span><div id="menus"><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">扩散模型（一）| DDPM &amp; DDIM</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-01-02T16:00:00.000Z" title="发表于 2024-01-03 00:00:00">2024-01-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-05-07T14:54:59.628Z" title="更新于 2024-05-07 22:54:59">2024-05-07</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Diffusion-model/">Diffusion model</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="扩散模型（一）| DDPM &amp; DDIM"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote>
<p>学习笔记，主要参考：</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/666552214">https://zhuanlan.zhihu.com/p/666552214</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=ifCDXFdeaaM">https://www.youtube.com/watch?v=ifCDXFdeaaM</a>(hong-yi Lee Diffusion Model原理剖析）</li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Zh411A72y/?spm_id_from=333.999.0.0&amp;vd_source=f16d47823aea3a6e77eec228119ddc27">https://www.bilibili.com/video/BV1Zh411A72y/?spm_id_from=333.999.0.0&amp;vd_source=f16d47823aea3a6e77eec228119ddc27</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/545764550/answer/2670611518">https://www.zhihu.com/question/545764550/answer/2670611518</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/627616358">https://zhuanlan.zhihu.com/p/627616358</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/639540034">https://zhuanlan.zhihu.com/p/639540034</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/599887666">https://zhuanlan.zhihu.com/p/599887666</a></li>
</ol>
</blockquote>
<h2 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h2><p>学习一个网络，使得该网络生成的图像集的分布和真实的图像集的分布越接近越好（生成图像集的分布和真实图像集的分布的KL散度越小越好）。从最大似然估计角度理解，希望生成的图像数据集的分布中产生真实图像数据集中的样本的概率越大越好。</p>
<p>如下图所示，从一个分布z中采样一个vector出来，通过网络，生成一张图片；所有采样到的vector生成的图片可以得到一个分布。目的是学习到这个网络，使生成图像的分布和真实图像分布接近。</p>
<img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/1-diffusion-models/17.PNG" alt="图像生成模型：从一个分布z中采样一个vector出来，通过网络，生成一张图片；所有采样到的vector生成的图片可以得到一个分布。目的是学习到这个网络，使生成图像的分布和真实图像分布接近。" style="zoom: 50%;" />

<h3 id="最大化最大似然估计-最小化KL散度"><a href="#最大化最大似然估计-最小化KL散度" class="headerlink" title="最大化最大似然估计=最小化KL散度"></a>最大化最大似然估计=最小化KL散度</h3><p>从分布 $z$中采样vector,送入网络 $\theta$产生 $x$，可以得到一个分布 $P_\theta(x)$。真实的训练数据集的分布是 $P_{data}(x)$，从真实的分布中采样 $x^1,x^2,..,x^m$，目的是让从学习到的分布 $P_\theta(x)$产生 $x^i$的概率最大。$P_\theta(x^i)$即分布 $P_\theta$产生 $x^i$的概率。</p>
<img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/1-diffusion-models/14.PNG" alt="img" style="zoom:67%;" />

<p>（推导思路：连乘-&gt;加log变成连加-&gt;转换成分布概率公式-&gt;减去真实分布-&gt;KL散度）</p>
<p>由上述推导可以得出最大化最大似然估计等价于最小化KL散度。</p>
<h2 id="DDPM"><a href="#DDPM" class="headerlink" title="DDPM"></a>DDPM</h2><h3 id="训练和推理算法流程"><a href="#训练和推理算法流程" class="headerlink" title="训练和推理算法流程"></a>训练和推理算法流程</h3><p><strong>训练</strong></p>
<p><img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/1-diffusion-models/0.jpg" alt="img"></p>
<p>总体来看，训练过程将图像添加噪声变成噪声图，之后将噪声图和时间步输入模型，模型来预测噪声。</p>
<p>具体来看，首先从图像数据集分布 $q(x_0)$中采样样本 $x_0$，从自然数集合中采样时间步 $t$，从高斯分布中采样噪声 $\epsilon$(大小和image相同）。生成添加噪声的新样本图$x_1 = \sqrt{\bar\alpha_t}x_0+\sqrt{1-\bar\alpha_t} \epsilon$。将新样本图和时间步 $t$送入噪声预测模型中，模型预测出噪声 $\epsilon_\theta$，预测的噪声和真实的噪声求损失，来更新模型参数。重复上述步骤直至收敛。</p>
<p><strong>推理</strong></p>
<p><img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/1-diffusion-models/1.jpg" alt="img"></p>
<p>总体来说，给定一个噪声图，推理过程每一步预测噪声图的噪声，将噪声图去噪还原成更接近原图的图，之后重复步骤，生成越来越清晰的图像。</p>
<p>具体来看，推理过程首先从高斯分布中采样一个噪声图 $x_T$。</p>
<p>之后从 $t=T$一直到 $t=1$重复 $T$步，每一步首先从高斯分布采样一个噪声 $z$，之后将 $x_t$和时间步 $t$送入噪声预测模型预测出噪声 $\epsilon_\theta$，根据预测的噪声和图像 $x_t$以及采样的噪声 $z$用公式求出 $x_{t-1}$。</p>
<h3 id="训练过程原理"><a href="#训练过程原理" class="headerlink" title="训练过程原理"></a>训练过程原理</h3><blockquote>
<p> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/666552214">https://zhuanlan.zhihu.com/p/666552214</a></p>
</blockquote>
<p> 训练过程对应前向加噪过程。一步步给图像添加噪音，使其变模糊。当步骤足够多，图像接近一张纯噪声。</p>
<p>$x_0$是从训练图像数据分布 $q(x_0)$中采样的样本。</p>
<p>从第 $x_{t-1}$张图生成第 $x_t$张图的公式： $x_t = \sqrt{1-\beta_t}x_{t-1}+\sqrt{\beta_t}\epsilon，\epsilon \sim N(0,1)$，其中 $\beta_1,\beta_2,…\beta_t$是一组人为设定的固定常数，随着 $t$的增加而增大。</p>
<p>令 $\alpha_t = 1 - \beta_t$，则公式变为 $x_t = \sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}\epsilon$</p>
<p>继续推导，得到 </p>
<p>$$\begin{align*}x_t &amp;= \sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}\epsilon \\&amp;= \sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}x_{t-2}+\sqrt{1-\alpha_{t-1}}\epsilon)+\sqrt{1-\alpha_t}\epsilon\\&amp;=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+\sqrt{\alpha_t(1-\alpha_{t-1})}\epsilon +\sqrt{1-\alpha_t}\epsilon\end{align*}$$</p>
<p>由于<strong>正态分布的可加性</strong>， $\sqrt{\alpha_t(1-\alpha_{t-1})}\epsilon +\sqrt{1-\alpha_t}\epsilon$可以看作：</p>
<p> $X_1 \in \sqrt{\alpha_t(1-\alpha_{t-1})}\epsilon =N(0,\alpha_t(1-\alpha_{t-1}))$</p>
<p> $X_2 \in \sqrt{1-\alpha_t}\epsilon = N(0,1-\alpha_t)$</p>
<p> $X_1+X_2 = N(0,1-\alpha_t\alpha_{t-1})$</p>
<p>即， $\sqrt{\alpha_t(1-\alpha_{t-1})}\epsilon +\sqrt{1-\alpha_t}\epsilon = \sqrt{1-\alpha_t\alpha_{t-1}}\epsilon$</p>
<p>所以， $x_t = \sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+\sqrt{1-\alpha_t\alpha_{t-1}}\epsilon$</p>
<p>数学归纳法，进一步推导得：</p>
<p> $x_t = \sqrt{\alpha_t\alpha_{t-1}…\alpha_1}x_0 + \sqrt{1-\alpha_t\alpha_{t-1}…\alpha_1}\epsilon$</p>
<p>令 $\bar\alpha_t=\alpha_t\alpha_{t-1}…\alpha_1$，则公式进一步化简为</p>
<p>$x_t = \sqrt{\bar\alpha_t}x_0+\sqrt{1-\bar\alpha_t}\epsilon$</p>
<p>从而可以看出，仅仅由 $x_0$即可一步得到 $x_t$。</p>
<p>（推导过程思路：代入 $x_{t-1}$，展开式子，利用正态分布的可加性）</p>
<h3 id="推理过程原理"><a href="#推理过程原理" class="headerlink" title="推理过程原理"></a>推理过程原理</h3><blockquote>
<p> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/666552214">https://zhuanlan.zhihu.com/p/666552214</a></p>
</blockquote>
<p>推理过程即反向去噪过程，目的是根据当前时刻的图片去预测前一时刻的图片，逐步还原成原图。</p>
<p>数学角度来看，即需要我们求出 $p(x_{t-1}|x_t)$，在给定 $x_t$的情况下，预测 $x_{t-1}$的概率分布。（注意，不是预测 $x_{t-1}$，而是预测 $x_{t-1}$的概率分布，由于生成结果需要满足多样性， $x_{t-1}$可能是任何相关的图像，而不是某一个具体的图像。 $x_{t-1}$是某张图的概率可能是最大的，是另一张图的概率较小，但是也不排除会生成。要从分布的角度思考，而不是单一样本的角度来思考）</p>
<p>采用贝叶斯公式计算后验概率：</p>
<p> $P(x_{t-1}|x_t) = \frac{P(x_{t-1})P(x_t)}{P(x_t)} = \frac{P(x_t|x_{t-1})P(x_{t-1})}{P(x_t)}$</p>
<p>因为<strong>推理过程满足马尔可夫假设</strong>，$P(x_{t-1}|x_t) = P(x_{t-1}|x_t,x_0)$。（马尔可夫链：状态空间中经过一个状态到另一个状态的转换的随机过程，具备无记忆性，即<strong>下一状态的概率分布只能由当前状态决定</strong>。所以， $x_{t-1}$只能由 $x_t$决定，和 $x_0$无关，所以等式 $P(x_{t-1}|x_t) = P(x_{t-1}|x_t,x_0)$成立）</p>
<p> $P(x_{t-1}|x_t) = P(x_{t-1}|x_t,x_0)\ =\frac{P(x_t|x_{t-1},x_0)P(x_{t-1},x_0)}{P(x_t|x_0)}$</p>
<p>根据公式 $x_t = \sqrt{1-\beta_t}x_{t-1}+\sqrt{\beta_t}\epsilon$和 $x_t = \sqrt{\bar\alpha_t}x_0+\sqrt{1-\bar\alpha_t}\epsilon$，得到：</p>
<p> $P(x_t|x_{t-1},x_0) = P(x_t|x_{t-1})=N(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_tI) = N(x_t;\sqrt{\alpha_t}x_{t-1},(1-\alpha_t)I)$</p>
<p> $P(x_{t-1},x_0)=N(x_{t-1};\sqrt{\bar\alpha_{t-1}}x_0,(1-\bar\alpha_{t-1})I)$</p>
<p> $P(x_t|x_0)=N(x_t;\sqrt{\bar\alpha_t}x_0,(1-\bar\alpha_t)I)$</p>
<p>为此， $P(x_{t-1}|x_t)  =\frac{P(x_t|x_{t-1},x_0)P(x_{t-1},x_0)}{P(x_t|x_0)} = \frac{N(x_t;\sqrt{\alpha_t}x_{t-1},(1-\alpha_t)I)N(x_{t-1};\sqrt{\bar\alpha_{t-1}}x_0,(1-\bar\alpha_{t-1})I)}{N(x_t;\sqrt{\bar\alpha_t}x_0,(1-\bar\alpha_t)I)}$</p>
<p>由正态分布的概率密度函数： $f(x) = \frac{1}{\sqrt{2\pi\sigma}}exp(-\frac{(x-u)^2}{2\sigma^2})$</p>
<p>得到， $P(x_{t-1}|x_t,x_0)\propto exp(-\frac{1}{2}[\frac{(x_t-\sqrt{\alpha_t}x_{t-1})^2}{1-\alpha_t}+\frac{(x_{t-1}-\sqrt{\bar\alpha_{t-1}}x_0)^2}{1-\bar\alpha_{t-1}}-\frac{(x_t-\sqrt{\bar\alpha_t}x_0)^2}{1-\bar\alpha_t}]$</p>
<p>由于 $x_{t-1}$是我们关注的变量，整理成 $x_{t-1}$的形式：</p>
<p><img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/1-diffusion-models/2.jpg" alt="img"></p>
<p>由正态分布满足 $f(x) \propto exp -\frac{x^2+u^2-2xu}{\sigma^2}$，则：</p>
<p><img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/1-diffusion-models/3.jpg" alt="img"></p>
<p>又因为 $x_t = \sqrt{\bar\alpha_t}x_0+\sqrt{1-\bar\alpha_t}\epsilon$，把u里面的 $x_0$换掉，得：</p>
<p> $u = \frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon)$</p>
<p>所以， $P（x_{t-1}|x_t) = N(x_{t-1};\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon),\frac{(1-\alpha_t)(1-\bar\alpha_{t-1})}{1-\bar\alpha_t})$</p>
<p>即基于 $x_t$预测 $x_{t-1}$的分布是一个高斯分布，均值为 $\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon)$，方差为 $\frac{(1-\alpha_t)(1-\bar\alpha_{t-1})}{1-\bar\alpha_t}$。推理算法中第4行即代表这个分布。</p>
<p>（推导过程思路：贝叶斯公式展开 $P(x_{t-1}|x_t)$-&gt;马尔可夫假设添加 $x_0$-&gt;正态分布性质进行化简-&gt;得到的也是一个正态分布，得到 $u$和 $\sigma$，即得出需要学习的分布）</p>
<h3 id="VAE与DDPM"><a href="#VAE与DDPM" class="headerlink" title="VAE与DDPM"></a>VAE与DDPM</h3><h4 id="相同和不同点"><a href="#相同和不同点" class="headerlink" title="相同和不同点"></a>相同和不同点</h4><ul>
<li>相同之处：<ul>
<li>前向过程都是将数据转化为一系列潜在表示；反向去噪过程都是把潜在表示生成原图像。</li>
<li>训练目标是最大似然的下界</li>
</ul>
</li>
<li>不同之处：<ul>
<li>DDPM可以看作是层次化的VAE，从一步到位改成T步到位，一个包含T个隐变量的隐变量模型</li>
<li>DDPM中的正向过程是固定好的人为设计的encoder，把原图变成噪声图的过程不是学习得到的</li>
<li>DDPM中潜在噪声图（隐变量）的维度和图像本身相同，而VAE中潜在空间一般会降低维度</li>
</ul>
</li>
</ul>
<p><img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/1-diffusion-models/4.jpg" alt="img"></p>
<h4 id="变分下界角度推导VAE和DDPM"><a href="#变分下界角度推导VAE和DDPM" class="headerlink" title="变分下界角度推导VAE和DDPM"></a>变分下界角度推导VAE和DDPM</h4><blockquote>
<p> 变分下界：variational lower bound, VLB</p>
</blockquote>
<ol>
<li>VAE</li>
</ol>
<p>推导如下，其中，因为 $\int_zq(z|x)dz=1$，所以第一行 $logP_\theta(x)=\int_zq(z|x)logP(x)dz$成立。</p>
<p>第二行利用贝叶斯公式展开，分子分母上下添加 $q(z|x)$。</p>
<p>第三行展开，得到右边是一项KL散度，KL散度一定大于等于0，则得到下界。</p>
<img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/1-diffusion-models/15.png" alt="img" style="zoom:67%;" />

<ol start="2">
<li>DDPM</li>
</ol>
<img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/1-diffusion-models/16.png" alt="img" style="zoom:50%;" />

<p>进一步化简:</p>
<p><img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/1-diffusion-models/5.jpg" alt="img"></p>
<p>即优化下面这个式子，让这个式子越大越好：</p>
<img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/1-diffusion-models/6.jpg" alt="img" style="zoom:67%;" />

<p>其中该等式第二项是diffusion的前向过程，不是网络学习到的，所以可以不看；第一项和第三项的计算过程很像，以第三项为例，由“推理过程原理部分”推导出 $q(x_{t-1}|x_t,x_0)$是一个高斯分布，满足 $N（\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon),\frac{(1-\alpha_t)(1-\bar\alpha_{t-1})}{1-\bar\alpha_t}）$。为了使最大似然概率更大，则应该使第三项KL散度越小越好，则 $P(x_{t-1}|x_t)$应该尽可能和分布 $N（\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon),\frac{(1-\alpha_t)(1-\bar\alpha_{t-1})}{1-\bar\alpha_t}）$类似。得出和“推理过程原理”部分相同的结论。</p>
<h3 id="模型设计"><a href="#模型设计" class="headerlink" title="模型设计"></a>模型设计</h3><p>扩散模型的核心在于训练噪音预测模型，采用一个基于residual block和attention block的U-Net模型。</p>
<p><img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/1-diffusion-models/7.jpg" alt="img"></p>
<p>U-Net属于encoder-decoder架构。每个stage包含2个residual block，部分stage还加入了自注意力模块增加网络全局建模能力。添加time embedding模块将时间步编码到网络中（采用和transformer相同的正余弦函数编码方式），具体来说，DDPM在各个残差块都引入了time embedding。</p>
<ul>
<li>encoder：逐步压缩图像大小</li>
<li>decoder：将encoder压缩的特征逐渐恢复，decoder模块中还引入了跳跃连接，即concat了encoder中间得到的同维度特征，有利于网络优化</li>
</ul>
<p>噪音预测采用MSE平方误差损失。</p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/xiaohu2015/nngen/blob/main/models/diffusion_models/ddpm_mnist.ipynb">https://github.com/xiaohu2015/nngen/blob/main/models/diffusion_models/ddpm_mnist.ipynb</a></p>
</blockquote>
<h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><p>关键代码：</p>
<p>（1）对batch里每个样本随机采样一个时间步t</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t = torch.randint(<span class="number">0</span>,timesteps,(batch_size,),device=device).long()</span><br></pre></td></tr></table></figure>

<p>（2）预测噪音并计算损失</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">loss = gaussian_diffusion.train_losses(model,images,t)  <span class="comment">#images [b,c,h,w]; t [b]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_losses</span>(<span class="params">self,model,x_start,t</span>):</span><br><span class="line">    <span class="comment">#采样随机噪声</span></span><br><span class="line">    noise = torch.randn_like(x_start) <span class="comment">#[b,c,h,w]</span></span><br><span class="line">    <span class="comment">#得到噪声图</span></span><br><span class="line">    x_noisy = self.q_sample(x_start,t,noise=noise)</span><br><span class="line">    <span class="comment">#噪声图和t送入模型预测噪声</span></span><br><span class="line">    predicted_noise = model(x_noisy,t)</span><br><span class="line">    <span class="comment">#计算预测噪声和真实噪声的mse损失</span></span><br><span class="line">    loss = F.mse_loss(noise,predicted_noise)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line">    </span><br><span class="line"> <span class="comment">#前向加噪过程</span></span><br><span class="line"> <span class="keyword">def</span> <span class="title function_">q_sample</span>(<span class="params">self,s_start,t,noise=<span class="literal">None</span></span>):</span><br><span class="line">     <span class="keyword">if</span> noise <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">         noise = torch.randn_like(x_start)</span><br><span class="line">     sqrt_alphas_cumprod_t = self._extract(self.sqrt_alphas_cumprod,t,x_start.shape) <span class="comment">#[b,1,1,1]</span></span><br><span class="line">     sqrt_one_minus_alphas_cumprod_t = self._extract(self.sqrt_one_minus_alphas_cumprod,t,x_start.shape) <span class="comment">#[b,1,1,1]</span></span><br><span class="line">     </span><br><span class="line">     <span class="keyword">return</span> sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise</span><br></pre></td></tr></table></figure>

<p>其中代码 <code>predicted_noise = model(x_noisy,t)</code>将时间步t和噪声图x_noisy一起送入噪声预测模型中。时间步t嵌入到噪声预测模型中的方式：</p>
<ul>
<li><p>timesteps([b])首先被正余弦位置编码编码([b,model_channels])，之后经过2个linear层([b,model_channels*4])</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">emb = self.time_embed(timestep_embedding(timesteps,self.model_channels))</span><br></pre></td></tr></table></figure></li>
<li><p>正余弦位置编码</p>
<p>和transformer类似，transformer中的位置编码函数：</p>
<img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/03/29/4.png" alt="img" style="zoom:50%;" />

<p>相当于偶数位置采用sin函数，奇数位置采用cos函数。其中公共部分可以如下推导：</p>
<p>$10000^{\frac{2i}{d}} = e^{log(10000^{\frac{2i}{d}})} = e^{\frac{2i}{d}log1000}$</p>
<p>此外，ddpm中采用每个位置的dim维度中前一半维度使用cos编码，后一半维度使用sin编码的方式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># use sinusoidal position embedding to encode time step (https://arxiv.org/abs/1706.03762)   </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">timestep_embedding</span>(<span class="params">timesteps, dim, max_period=<span class="number">10000</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Create sinusoidal timestep embeddings.</span></span><br><span class="line"><span class="string">    :param timesteps: a 1-D Tensor of N indices, one per batch element.</span></span><br><span class="line"><span class="string">                      These may be fractional.</span></span><br><span class="line"><span class="string">    :param dim: the dimension of the output.</span></span><br><span class="line"><span class="string">    :param max_period: controls the minimum frequency of the embeddings.</span></span><br><span class="line"><span class="string">    :return: an [N x dim] Tensor of positional embeddings.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    half = dim // <span class="number">2</span></span><br><span class="line">    freqs = torch.exp(</span><br><span class="line">        -math.log(max_period) * torch.arange(start=<span class="number">0</span>, end=half, dtype=torch.float32) / half</span><br><span class="line">    ).to(device=timesteps.device)</span><br><span class="line">    args = timesteps[:, <span class="literal">None</span>].<span class="built_in">float</span>() * freqs[<span class="literal">None</span>]</span><br><span class="line">    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> dim % <span class="number">2</span>:</span><br><span class="line">        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :<span class="number">1</span>])], dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> embedding</span><br></pre></td></tr></table></figure></li>
<li><p>time_embed层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">time_embed_dim = model_channels * <span class="number">4</span></span><br><span class="line">self.time_embed = nn.Sequential(</span><br><span class="line">     nn.Linear(model_channels, time_embed_dim),</span><br><span class="line">     nn.SiLU(),</span><br><span class="line">     nn.Linear(time_embed_dim, time_embed_dim),</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li>
<li><p>时间步信息在每个残差块注入</p>
<p>采用将时间步嵌入和图像特征直接相加进行融合的方式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># Residual block</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualBlock</span>(<span class="title class_ inherited__">TimestepBlock</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, time_channels, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            norm_layer(in_channels),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># pojection for time step embedding</span></span><br><span class="line">        self.time_emb = nn.Sequential(</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            nn.Linear(time_channels, out_channels)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.conv2 = nn.Sequential(</span><br><span class="line">            norm_layer(out_channels),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            nn.Dropout(p=dropout),</span><br><span class="line">            nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> in_channels != out_channels:</span><br><span class="line">            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.shortcut = nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        `x` has shape `[batch_size, in_dim, height, width]`</span></span><br><span class="line"><span class="string">        `t` has shape `[batch_size, time_dim]`</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        h = self.conv1(x)</span><br><span class="line">        <span class="comment"># Add time step embeddings</span></span><br><span class="line">        h += self.time_emb(t)[:, :, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">        h = self.conv2(h)</span><br><span class="line">        <span class="keyword">return</span> h + self.shortcut(x)</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="推理过程"><a href="#推理过程" class="headerlink" title="推理过程"></a>推理过程</h4><p>（1）生成纯噪音图像</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img = torch.randn(shape,device=device)  <span class="comment">#img [b,c,h,w]</span></span><br></pre></td></tr></table></figure>

<p>（2）循环timesteps个时间步，逐渐更新图像</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">imgs = [] <span class="comment">#一个len = timesteps的数组，数组里每一项的维度为[b,c,h,w],代表迭代过程每一步的batch里各个图像的样子</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="number">0</span>,timesteps)),desc=<span class="string">&#x27;sampling loop time step&#x27;</span>,total=timesteps):</span><br><span class="line">    <span class="comment">#torch.full函数构造时间步[b]，值为当前的时间i</span></span><br><span class="line">    img = self.p_sample(model,img,torch.full((batch_size,),i,device=device,dtype=torch.long))</span><br><span class="line">    imgs.append(img.cpu().numpy())</span><br><span class="line"><span class="keyword">return</span> imgs</span><br></pre></td></tr></table></figure>

<p>（3）每个时间步内操作</p>
<ul>
<li><p>整体流程</p>
<p>首先计算均值和对数方差；采样噪音，其中因为噪音在最后一步的时候为0，所以使用一个nonzero_mask矩阵来判断</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># denoise_step: sample x_&#123;t-1&#125; from x_t and pred_noise</span></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p_sample</span>(<span class="params">self, model, x_t, t, clip_denoised=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># predict mean and variance</span></span><br><span class="line">    model_mean, _, model_log_variance = self.p_mean_variance(model, x_t, t,</span><br><span class="line">                                                clip_denoised=clip_denoised)</span><br><span class="line">    noise = torch.randn_like(x_t)</span><br><span class="line">    <span class="comment"># no noise when t == 0</span></span><br><span class="line">    nonzero_mask = ((t != <span class="number">0</span>).<span class="built_in">float</span>().view(-<span class="number">1</span>, *([<span class="number">1</span>] * (<span class="built_in">len</span>(x_t.shape) - <span class="number">1</span>))))</span><br><span class="line">    <span class="comment"># compute x_&#123;t-1&#125;</span></span><br><span class="line">    pred_img = model_mean + nonzero_mask * (<span class="number">0.5</span> * model_log_variance).exp() * noise</span><br><span class="line">    <span class="keyword">return</span> pred_img</span><br></pre></td></tr></table></figure></li>
<li><p>“计算 $p(x_{t-1}|x_t)$的均值和方差”步骤</p>
<p>推理算法中给出的公式如下，实际实现时采用的公式是未将 $x_0$替换的版本，并且对 $x_0$做了clip操作（将 $x_0$中的元素限制在-1和1之间），使用的是对数方差再取指数e（对数方差限制最小值为0）：</p>
<img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/03/29/9.png" alt="img" style="zoom:67%;" />

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">model_mean, _, model_log_variance = self.p_mean_variance(model,x_t,t,</span><br><span class="line">                                                     clip_denoised=clip_denoised)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#self.p_mean_variance函数</span></span><br><span class="line"><span class="comment"># compute predicted mean and variance of p(x_&#123;t-1&#125; | x_t)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p_mean_variance</span>(<span class="params">self, model, x_t, t, clip_denoised=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># predict noise using model</span></span><br><span class="line">    pred_noise = model(x_t, t)</span><br><span class="line">    <span class="comment"># get the predicted x_0</span></span><br><span class="line">    x_recon = self.predict_start_from_noise(x_t, t, pred_noise)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> clip_denoised:</span><br><span class="line">        x_recon = torch.clamp(x_recon, <span class="built_in">min</span>=-<span class="number">1.</span>, <span class="built_in">max</span>=<span class="number">1.</span>)</span><br><span class="line">    model_mean, posterior_variance, posterior_log_variance = \</span><br><span class="line">                self.q_posterior_mean_variance(x_recon, x_t, t)</span><br><span class="line">    <span class="keyword">return</span> model_mean, posterior_variance, posterior_log_variance</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Compute the mean and variance of the diffusion posterior: q(x_&#123;t-1&#125; | x_t, x_0)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">q_posterior_mean_variance</span>(<span class="params">self, x_start, x_t, t</span>):</span><br><span class="line">    posterior_mean = (</span><br><span class="line">        self._extract(self.posterior_mean_coef1, t, x_t.shape) * x_start</span><br><span class="line">        + self._extract(self.posterior_mean_coef2, t, x_t.shape) * x_t</span><br><span class="line">    )</span><br><span class="line">    posterior_variance = self._extract(self.posterior_variance, t, x_t.shape)</span><br><span class="line">    posterior_log_variance_clipped = self._extract(self.posterior_log_variance_clipped, t, x_t.shape)</span><br><span class="line">    <span class="keyword">return</span> posterior_mean, posterior_variance, posterior_log_variance_clipped</span><br><span class="line">    </span><br><span class="line">self.posterior_mean_coef1 = (</span><br><span class="line">    self.betas * torch.sqrt(self.alphas_cumprod_prev) / (<span class="number">1.0</span> - self.alphas_cumprod)</span><br><span class="line">)</span><br><span class="line">self.posterior_mean_coef2 = (</span><br><span class="line">    (<span class="number">1.0</span>-self.alphas_cumprod_prev)*torch.sqrt(self.alphas)/(<span class="number">1.0</span>-self.alphas_cumprod)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculations for posterior q(x_&#123;t-1&#125; | x_t, x_0)</span></span><br><span class="line">self.posterior_variance = (</span><br><span class="line">    self.betas * (<span class="number">1.0</span> - self.alphas_cumprod_prev) / (<span class="number">1.0</span> - self.alphas_cumprod)</span><br><span class="line">)</span><br><span class="line"><span class="comment"># below: log calculation clipped because the posterior variance is 0 at the beginning</span></span><br><span class="line"><span class="comment"># of the diffusion chain</span></span><br><span class="line">self.posterior_log_variance_clipped = torch.log(self.posterior_variance.clamp(<span class="built_in">min</span> =<span class="number">1e-20</span>))</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="可视化结果"><a href="#可视化结果" class="headerlink" title="可视化结果"></a>可视化结果</h4><ul>
<li>生成结果</li>
</ul>
<img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/03/29/6.png" alt="img" style="zoom: 33%;" />

<ul>
<li>逐步结果</li>
</ul>
<img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/03/29/7.png" alt="img" style="zoom: 33%;" />

<ul>
<li><p>使用重参数化公式 $x_t =\sqrt{\bar\alpha_t}x_0+\sqrt{1-\bar\alpha_t}\epsilon$一步计算得到 $x_0$的结果</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/583158958">https://www.zhihu.com/question/583158958</a></p>
</blockquote>
<p>可以看出，直接一步计算得到 $x_0$的结果是不行的。正向是一个加噪的过程，可以粗糙一点；但是逆向过程是一个复原图像的过程，需要更精细，否则误差会非常大。</p>
</li>
</ul>
<img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/03/29/8.png" alt="img" style="zoom: 25%;" />

<h2 id="DDIM"><a href="#DDIM" class="headerlink" title="DDIM"></a>DDIM</h2><p>DDPM的推理速度过慢，需要设置较长的扩散步数才能得到好的效果。其无法避免迭代过程，因为其本身是一个<strong>马尔可夫链</strong>，即前后时刻数据有非常紧密的绑定关系，无法进行跳跃预测。</p>
<p>DDIM（denoising diffusion implicit models)和DDPM有相同的训练目标，但是<strong>不再限制扩散过程必须是一个马尔可夫链</strong>，使得DDIM可以采用更大的采样步数来加速生成过程。DDIM的另外一个特点是当 $\sigma$取值为0时，从一个随机噪音生成样本的过程是一个<strong>确定的过程</strong>（中间没有加入随机噪音）。DDIM<strong>无需重新训练DDPM</strong>（无需改变前向加噪过程），只对采样器进行修改即可，修改后的采样器能够大幅增加采样速度。</p>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>由“变分下界推导VAE和DDPM”一节可以看出优化的终极目标是去拟合概率分布 $P(x_{t-1}|x_0,x_t)$，公式：</p>
<p> $P(x_{t-1}|x_t,x_0) = \frac{P(x_t|x_{t-1},x_0)P(x_{t-1}|x_0)}{P(x_t|x_0)}$</p>
<p>不再假设它是一个马尔可夫链，<strong>使用待定系数法</strong>，假设 $P(x_{t-1}|x_t,x_0)$满足正态分布：</p>
<p> $P(x_{t-1}|x_t,x_0) \sim N(kx_0+mx_t,\sigma^2I)$</p>
<p>则 $x_{t-1} = kx_0+mx_t+\sigma\epsilon$。</p>
<p>又由于前向过程公式 $x_t = \sqrt{\bar\alpha_t}x_0+\sqrt{1-\bar\alpha_t}\epsilon$，</p>
<p>得：  $x_{t-1} = kx_0+m(\sqrt{\bar\alpha_t}x_0+\sqrt{1-\bar\alpha_t}\epsilon)+\sigma\epsilon=(k+m\sqrt{\bar\alpha_t})x_0+m\sqrt{1-\bar\alpha_t}\epsilon+\sigma\epsilon$</p>
<p>由正态分布的可加性，得 $x_{t-1}=(k+m\sqrt{\bar\alpha_t})x_0+\sqrt{m^2(1-\bar\alpha_t)+\sigma^2}\epsilon$</p>
<p>又 $x_{t-1} = \sqrt{\bar\alpha_{t-1}}x_0+\sqrt{1-\bar\alpha_{t-1}}\epsilon$，对应系数相同，得到：</p>
<p> $k+m\sqrt{\bar\alpha_t} = \sqrt{\bar\alpha_{t-1}}$, $m^2(1-\bar\alpha_t)+\sigma^2=1-\bar\alpha_{t-1}$</p>
<p>解方程组，得：</p>
<p>因此， $P(x_{t-1}|x_t,x_0)=N(\sqrt{\bar\alpha_{t-1}}x_0+\sqrt{1-\bar\alpha_{t-1}-\sigma^2}\frac{x_t-\sqrt{\bar\alpha_t}x_0}{\sqrt{1-\bar\alpha_t}},\sigma^2I)$</p>
<p>用 $x_t$替换 $x_0$，得到：</p>
<p> $x_{t-1} = \sqrt{\bar\alpha_{t-1}}(\frac{x_t-\sqrt{1-\bar\alpha_t}\epsilon_\theta(x_t)}{\sqrt{\bar\alpha_t}})+\sqrt{1-\bar\alpha_{t-1}-\sigma^2}\epsilon_\theta(x_t)+\sigma\epsilon$</p>
<p>由于推导过程中没有使用马尔可夫性质，所以可以不满足马尔可夫要求， $t-1$可以替换为 $prev$， $x_t$和 $x_{prev}$可以相隔多个迭代步数。</p>
<p>（推导思路：使用待定系数法假设它满足一个分布，之后结合前向过程公式可以推导出系数）</p>
<h3 id="方差的取值"><a href="#方差的取值" class="headerlink" title="方差的取值"></a>方差的取值</h3><p> $\sigma$的取值不会影响推导式子的成立。</p>
<ul>
<li>当 $\sigma = 0$，采样过程不再具有随机性，每个 $x_T$对应了确定的 $x_0$。</li>
<li>若 $\sigma = \frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}\beta_t$（  $= \frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}(1-\frac{\bar\alpha_t}{\bar\alpha_{t-1}})$)，则是DDPM中采用的方差，此时<strong>DDIM等价于DDPM</strong>。</li>
</ul>
<img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/03/29/10.png" alt="img" style="zoom: 50%;" />

<img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/03/29/11.png" alt="img" style="zoom:50%;" />

<h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><blockquote>
<p> <a target="_blank" rel="noopener" href="https://github.com/xiaohu2015/nngen/blob/main/models/diffusion_models/ddim_mnist.ipynb">https://github.com/xiaohu2015/nngen/blob/main/models/diffusion_models/ddim_mnist.ipynb</a></p>
</blockquote>
<ul>
<li>训练过程和ddpm一致</li>
<li>测试过程函数：</li>
</ul>
<p>（1）因为ddim可以隔很多步进行采样，所以时间序列不再是[499,498,497,496]，而是类似[476,451,426,376,…]的间隔序列，可以采用均匀采样的方式采样间隔的时间步：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment"># make ddim timestep sequence</span></span><br><span class="line"><span class="keyword">if</span> ddim_discr_method == <span class="string">&#x27;uniform&#x27;</span>:</span><br><span class="line">    c = self.timesteps // ddim_timesteps</span><br><span class="line">    ddim_timestep_seq = np.asarray(<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">0</span>, self.timesteps, c)))</span><br><span class="line"><span class="keyword">elif</span> ddim_discr_method == <span class="string">&#x27;quad&#x27;</span>:</span><br><span class="line">    ddim_timestep_seq = (</span><br><span class="line">        (np.linspace(<span class="number">0</span>, np.sqrt(self.timesteps * <span class="number">.8</span>), ddim_timesteps)) ** <span class="number">2</span></span><br><span class="line">    ).astype(<span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError(<span class="string">f&#x27;There is no ddim discretization method called &quot;<span class="subst">&#123;ddim_discr_method&#125;</span>&quot;&#x27;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># add one to get the final alpha values right (the ones from first scale to data during sampling)</span></span><br><span class="line">ddim_timestep_seq = ddim_timestep_seq + <span class="number">1</span></span><br><span class="line"><span class="comment"># previous sequence</span></span><br><span class="line">ddim_timestep_prev_seq = np.append(np.array([<span class="number">0</span>]), ddim_timestep_seq[:-<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>（2）采样噪声图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># start from pure noise (for each example in the batch)</span></span><br><span class="line">sample_img = torch.randn((batch_size, channels, image_size, image_size), device=device)</span><br></pre></td></tr></table></figure>

<p>（3）开始迭代还原图像，根据公式 $x_{t-1} = \sqrt{\bar\alpha_{t-1}}(\frac{x_t-\sqrt{1-\bar\alpha_t}\epsilon_\theta(x_t)}{\sqrt{\bar\alpha_t}})+\sqrt{1-\bar\alpha_{t-1}-\sigma^2}\epsilon_\theta(x_t)+\sigma\epsilon$，其中方差的公式采用 $\sigma = \eta\sqrt{\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}}\sqrt{1-\frac{\bar\alpha_t}{\bar\alpha_{t-1}}}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">sample_img = torch.randn((batch_size, channels, image_size, image_size), device=device)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="number">0</span>, ddim_timesteps)), desc=<span class="string">&#x27;sampling loop time step&#x27;</span>, total=ddim_timesteps):</span><br><span class="line">    t = torch.full((batch_size,), ddim_timestep_seq[i], device=device, dtype=torch.long)</span><br><span class="line">    prev_t = torch.full((batch_size,), ddim_timestep_prev_seq[i], device=device, dtype=torch.long)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 1. get current and previous alpha_cumprod</span></span><br><span class="line">    alpha_cumprod_t = self._extract(self.alphas_cumprod, t, sample_img.shape)</span><br><span class="line">    alpha_cumprod_t_prev = self._extract(self.alphas_cumprod, prev_t, sample_img.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. predict noise using model</span></span><br><span class="line">    pred_noise = model(sample_img, t)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. get the predicted x_0</span></span><br><span class="line">    pred_x0 = (sample_img - torch.sqrt((<span class="number">1.</span> - alpha_cumprod_t)) * pred_noise) / torch.sqrt(alpha_cumprod_t)</span><br><span class="line">    <span class="keyword">if</span> clip_denoised:</span><br><span class="line">        pred_x0 = torch.clamp(pred_x0, <span class="built_in">min</span>=-<span class="number">1.</span>, <span class="built_in">max</span>=<span class="number">1.</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 4. compute variance: &quot;sigma_t(η)&quot; -&gt; see formula (16)</span></span><br><span class="line">    <span class="comment"># σ_t = sqrt((1 − α_t−1)/(1 − α_t)) * sqrt(1 − α_t/α_t−1)</span></span><br><span class="line">    sigmas_t = ddim_eta * torch.sqrt(</span><br><span class="line">        (<span class="number">1</span> - alpha_cumprod_t_prev) / (<span class="number">1</span> - alpha_cumprod_t) * (<span class="number">1</span> - alpha_cumprod_t / alpha_cumprod_t_prev))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 5. compute &quot;direction pointing to x_t&quot; of formula (12)</span></span><br><span class="line">    pred_dir_xt = torch.sqrt(<span class="number">1</span> - alpha_cumprod_t_prev - sigmas_t**<span class="number">2</span>) * pred_noise</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 6. compute x_&#123;t-1&#125; of formula (12)</span></span><br><span class="line">    x_prev = torch.sqrt(alpha_cumprod_t_prev) * pred_x0 + pred_dir_xt + sigmas_t * torch.randn_like(sample_img)</span><br><span class="line"></span><br><span class="line">    sample_img = x_prev</span><br></pre></td></tr></table></figure>

<ul>
<li>20步，ddim_eta=0时的生成效果：</li>
</ul>
<img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/03/29/12.png" alt="img" style="zoom: 33%;" />

<h3 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h3><p><strong>采样一致性（sample consistency)</strong></p>
<p>当方差为0时，生成过程是确定的，只受 $x_T$影响。给定不同的 $x_T$，不同的采样步数下生成的图片都是类似的， $x_T$可以看作生成图片的隐编码信息。（在实际生成图片时可以控制 $x_T$不变，设置较小的采样步数，若生成的图片是想要的，再用更大的步数生成更精细的图片）。</p>
<p><img src="https://lichtung612.eos-beijing-1.cmecloud.cn/2024/1-diffusion-models/8.jpg" alt="img"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">梦游娃娃</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/posts/1-diffusion-models/">http://example.com/posts/1-diffusion-models/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">披星戴月思想家</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/DDPM/">DDPM</a></div><div class="post_share"><div class="social-share" data-image="/image/avatar.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/2-diffusion-models/" title="扩散模型（二）| IDDPM"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">扩散模型（二）| IDDPM</div></div></a></div><div class="next-post pull-right"><a href="/posts/1-nas/" title="NAS｜AutoFormer &amp; AutoFormer++"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">NAS｜AutoFormer &amp; AutoFormer++</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/image/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">梦游娃娃</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">14</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">24/03/20: Finding an internship is all you need.<br>
<strike>24/02/02: Sleep is all you need.</strike>
</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87"><span class="toc-number">1.</span> <span class="toc-text">优化目标</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E5%8C%96%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1-%E6%9C%80%E5%B0%8F%E5%8C%96KL%E6%95%A3%E5%BA%A6"><span class="toc-number">1.1.</span> <span class="toc-text">最大化最大似然估计&#x3D;最小化KL散度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DDPM"><span class="toc-number">2.</span> <span class="toc-text">DDPM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B"><span class="toc-number">2.1.</span> <span class="toc-text">训练和推理算法流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E5%8E%9F%E7%90%86"><span class="toc-number">2.2.</span> <span class="toc-text">训练过程原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E7%90%86%E8%BF%87%E7%A8%8B%E5%8E%9F%E7%90%86"><span class="toc-number">2.3.</span> <span class="toc-text">推理过程原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#VAE%E4%B8%8EDDPM"><span class="toc-number">2.4.</span> <span class="toc-text">VAE与DDPM</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%B8%E5%90%8C%E5%92%8C%E4%B8%8D%E5%90%8C%E7%82%B9"><span class="toc-number">2.4.1.</span> <span class="toc-text">相同和不同点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%98%E5%88%86%E4%B8%8B%E7%95%8C%E8%A7%92%E5%BA%A6%E6%8E%A8%E5%AF%BCVAE%E5%92%8CDDPM"><span class="toc-number">2.4.2.</span> <span class="toc-text">变分下界角度推导VAE和DDPM</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1"><span class="toc-number">2.5.</span> <span class="toc-text">模型设计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81"><span class="toc-number">2.6.</span> <span class="toc-text">代码</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-number">2.6.1.</span> <span class="toc-text">训练过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8E%A8%E7%90%86%E8%BF%87%E7%A8%8B"><span class="toc-number">2.6.2.</span> <span class="toc-text">推理过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96%E7%BB%93%E6%9E%9C"><span class="toc-number">2.6.3.</span> <span class="toc-text">可视化结果</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DDIM"><span class="toc-number">3.</span> <span class="toc-text">DDIM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86"><span class="toc-number">3.1.</span> <span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E5%B7%AE%E7%9A%84%E5%8F%96%E5%80%BC"><span class="toc-number">3.2.</span> <span class="toc-text">方差的取值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-1"><span class="toc-number">3.3.</span> <span class="toc-text">代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E6%80%A7"><span class="toc-number">3.4.</span> <span class="toc-text">特性</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/11-diffusion-models/" title="扩散模型（十一）| LoRA">扩散模型（十一）| LoRA</a><time datetime="2024-02-27T16:00:00.000Z" title="发表于 2024-02-28 00:00:00">2024-02-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/10-diffusion-models/" title="扩散模型（十）| Transformer-based Diffusion：DiT">扩散模型（十）| Transformer-based Diffusion：DiT</a><time datetime="2024-02-19T16:00:00.000Z" title="发表于 2024-02-20 00:00:00">2024-02-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/9-diffusion-models/" title="扩散模型（九）| Transformer-based Diffusion：U-ViT">扩散模型（九）| Transformer-based Diffusion：U-ViT</a><time datetime="2024-01-29T16:00:00.000Z" title="发表于 2024-01-30 00:00:00">2024-01-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/8-diffusion-models/" title="扩散模型（八）| 3D Diffusion：PointE">扩散模型（八）| 3D Diffusion：PointE</a><time datetime="2024-01-23T16:00:00.000Z" title="发表于 2024-01-24 00:00:00">2024-01-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/7-diffusion-models/" title="扩散模型（七）| ControlNet">扩散模型（七）| ControlNet</a><time datetime="2024-01-18T16:00:00.000Z" title="发表于 2024-01-19 00:00:00">2024-01-19</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/image/cover.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By 梦游娃娃</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>